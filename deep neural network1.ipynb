{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "995c7767-57c6-44b0-917a-41a3af18ca30",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from keras.datasets import mnist\n",
    "(X_train,y_train), (X_test,y_test)= mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d60924c6-4b65-41da-8d23-6f6fe0adb69f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FC:\n",
    "    \n",
    "    def __init__(self, n_nodes1, n_nodes2, initializer, optimizer=None):\n",
    "        self.n_nodes1 = n_nodes1\n",
    "        self.n_nodes2 = n_nodes2\n",
    "        # Initialize\n",
    "        # Initialize self.W and self.B using the initializer method\n",
    "        self.W = initializer.W(self.n_nodes1, self.n_nodes2)\n",
    "        self.B = initializer.B(self.n_nodes2)\n",
    "        self.optimizer = optimizer\n",
    "\n",
    "        self.HW = 0\n",
    "        self.HB = 0\n",
    "        \n",
    "    def forward(self, X):\n",
    "        self.Z = X\n",
    "        self.A = X @ self.W + self.B\n",
    "        return self.A\n",
    "    \n",
    "    def backward(self, dA):\n",
    "        self.dB = np.sum(dA, axis=0) \n",
    "        self.dW = self.Z.T @ dA \n",
    "        self.dZ = dA @ self.W.T\n",
    "        \n",
    "        # Update weights and biases using the optimizer\n",
    "        #self.W, self.B = self.optimizer.update(self.W, self.B, self.dW, self.dB)\n",
    "            \n",
    "        return self.dZ\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "239b00e9-1d46-4fa4-ab94-d5edbfa7b9a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#problem 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ccee9d84-962d-49f1-8569-f25a0ca95dd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleInitializer:\n",
    "    \n",
    "    def __init__(self, sigma):\n",
    "        self.sigma = sigma\n",
    "    def W(self, n_nodes1, n_nodes2):\n",
    "        W = self.sigma * np.random.randn(n_nodes1, n_nodes2)\n",
    "        return W\n",
    "    def B(self, n_nodes2):\n",
    "        B = self.sigma * np.random.randn(1, n_nodes2)\n",
    "        return B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "87962d4b-7fb5-42d1-9e15-6b3ffa6da358",
   "metadata": {},
   "outputs": [],
   "source": [
    "#PROBLEM 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "205f7817-b179-4f81-8c27-2f9e9ce625f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SGD:\n",
    "   \n",
    "    def __init__(self, lr):\n",
    "        self.lr = lr\n",
    "    def update(self, layer):\n",
    "        layer.W-= self.lr * layer.dW/len(layer.Z)\n",
    "        layer.B -= self.lr + layer.dB/len(layer.Z)\n",
    "        return layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0cfb4ee6-3c7a-4991-bc20-d0c993f3ede7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Problem 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8a9095c0-c431-42bc-ba22-e668179d318d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sigmoid:\n",
    "    def forward(self, A):\n",
    "        self.A = A\n",
    "        Z= 1/(1+np.exp(-self.A))\n",
    "        return Z\n",
    "    def backward (self, dZ):\n",
    "        dA = dZ * ((1/(1+np.exp(-self.A)))- (1/(1+np.exp(-self.A)))**2)\n",
    "        return dA\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4eb1ab8f-7867-41ed-bded-e1ab214f400b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tanh:\n",
    "    def forward(self, A):\n",
    "        self.A = A\n",
    "        Z = np.tanh(self.A)\n",
    "        return Z\n",
    "    def backward (self, dZ):\n",
    "        dA = dZ * (1-np.tanh(self.A)**2)\n",
    "        return dA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e8cd822f-a640-4a90-8744-ba2e8e87c315",
   "metadata": {},
   "outputs": [],
   "source": [
    "class softmax:\n",
    "    def forward(self,A):\n",
    "        Z = np.exp(A) / np.sum(np.exp(A), axis=1).reshape(-1,1)\n",
    "        return Z\n",
    "    def backward(self, Z, y):\n",
    "        dA = Z-y\n",
    "        loss = -np.sum(y*np.log(Z)) / len(y)\n",
    "        return dA, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "aee5e4a3-a9d5-44d5-b723-5c0e2ebd8561",
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(z):\n",
    "    exp_scores = np.exp(z-np.max(z))\n",
    "    probabilities = exp_scores/np.sum(exp_scores, axis=1, keepdims=True)\n",
    "    return probabilities\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6b8f2458-9805-441a-b354-eb7cfdb8cf33",
   "metadata": {},
   "outputs": [],
   "source": [
    "#problem 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "76c0b03a-adcc-438b-8605-840092cfd49d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReLU:\n",
    "    def forward(self, A):\n",
    "        self.A = A\n",
    "        Z= np.maximum(0,A)\n",
    "        return Z\n",
    "    def backward(self, dZ):\n",
    "        dA = dZ * np.where(self.A >0,1,0)\n",
    "        return dA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "eb4d0a47-4c63-44d8-801e-4a2981356394",
   "metadata": {},
   "outputs": [],
   "source": [
    "#problem 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "859ea92b-4443-416d-8252-6045941e9c34",
   "metadata": {},
   "outputs": [],
   "source": [
    "class XavierInitializer:\n",
    "    def __init__(self,sigma):\n",
    "        _= sigma\n",
    "    def W(self,n_nodes1, n_nodes2):\n",
    "        self.sigma =1/np.sqrt(n_nodes1)\n",
    "        W = self.sigma *np.random.randn(n_nodes1, n_nodes2)\n",
    "        return W\n",
    "    def B(self, n_nodes2):\n",
    "        B= self.sigma *np.random.randn(1,n_nodes2)\n",
    "        return B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ae1770fb-b194-499a-ad06-e16b367c96bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HeInitializer:\n",
    "    def __init__(self,sigma):\n",
    "        _=sigma\n",
    "    def W(self, n_nodes1, n_nodes2):\n",
    "        self.sigma = np.sqrt(2/n_nodes1)\n",
    "        W= self.sigma* np.random.rand(n_nodes1, n_nodes2)\n",
    "        return W\n",
    "    def B(self, n_nodes2):\n",
    "        B= self.sigma * np.random.randn(1, n_nodes2)\n",
    "        return B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7ae6f6aa-ab01-4019-9136-89582e6eaec8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#problem 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fc205116-40b4-4d1e-b123-405cf211aff3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdaGrad:\n",
    "    def __init__(self, lr):\n",
    "        self.lr = lr\n",
    "\n",
    "    def update(self, layer):\n",
    "        layer.HW += layer.dW * layer.dW\n",
    "        layer.HB += layer.dB * layer.dB\n",
    "        delta =1e-7\n",
    "        layer.W -= self.lr * layer.dW / (np.sqrt(layer.HW) + delta)/len(layer.Z)\n",
    "        layer.B -= self.lr * layer.dB / (np.sqrt(layer.HB) + delta)/len(layer.Z)\n",
    "        return layer\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a0760bae-6a5d-40a0-a09e-c232e8a61b1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GetMiniBatch:\n",
    "    def __init__(self, X,y, batch_size =20, seed =0):\n",
    "        self.batch_size = batch_size\n",
    "        np.random.seed(seed)\n",
    "        shuffle_index = np.random.permutation(np.arange(X.shape[0]))\n",
    "        self._X = X[shuffle_index]\n",
    "        self._y = y[shuffle_index]\n",
    "        self._stop = np.ceil(X.shape[0]/self.batch_size).astype(int)\n",
    "        \n",
    "    def __len__ (self):\n",
    "        return self.stop\n",
    "        \n",
    "    def __getitem__(self,item):\n",
    "        p0 = item*self.batch_size\n",
    "        p1 = item*self.batch_size + self.batch_size\n",
    "        return self._X[p0:p1], self._y[p0:p1]\n",
    "        \n",
    "    def __iter__(self):\n",
    "        self._counter = 0\n",
    "        return self\n",
    "        \n",
    "    def __next__(self):\n",
    "        if self._counter >= self._stop:\n",
    "            raise StopIteration()\n",
    "        p0 = self._counter*self.batch_size\n",
    "        p1 = self._counter*self.batch_size + self.batch_size\n",
    "        self._counter +=1\n",
    "        return self._X[p0:p1], self._y[p0:p1]\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "30e5a571-3551-4c1d-9187-727376b91e62",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ScratchDeepNeuralNetworkClassifier():\n",
    "    def __init__(self, verbose=False, epoch =1, optimizer =SGD, initializer =HeInitializer, activater=ReLU):\n",
    "        self.verbose = verbose\n",
    "        self.batch_size = 20\n",
    "        self.n_features = 784\n",
    "        self.n_nodes1 = 400\n",
    "        self.n_nodes2 = 200\n",
    "        self.n_output = 10\n",
    "        self.sigma = 0.02\n",
    "        self.lr = 0.5\n",
    "        self.epoch = epoch\n",
    "        self.optimizer = optimizer\n",
    "        self.initializer = initializer\n",
    "        self.activater = activater\n",
    "        \n",
    "    def fit (self, X, y, X_val =None, y_val = None):\n",
    "        self.loss_train = []\n",
    "        self.loss_val=[]\n",
    "        optimizer = self.optimizer(self.lr)\n",
    "        self.FC1 =FC(self.n_features, self.n_nodes1, self.initializer(self.sigma), optimizer)\n",
    "        self.activation1 = self.activater()\n",
    "        self.FC2 = FC(self.n_nodes1, self.n_nodes2, self.initializer(self.sigma), optimizer)\n",
    "        self.activation2 = self.activater()\n",
    "        self.FC3 = FC(self.n_nodes2, self.n_output, self.initializer(self.sigma), optimizer)\n",
    "        self.activation3 = softmax()\n",
    "\n",
    "        for i in range(self.epoch):\n",
    "            get_mini_batch = GetMiniBatch(X,y, batch_size =self.batch_size, seed=i)\n",
    "            for mini_X, mini_y in get_mini_batch:\n",
    "                A1 = self.FC1.forward(mini_X)\n",
    "                Z1 = self.activation1.forward(A1)\n",
    "                A2 = self.FC2.forward(Z1)\n",
    "                Z2 = self.activation2.forward(A2)\n",
    "                A3 = self.FC3.forward(Z2)\n",
    "                Z3 = self.activation3.forward(A3)\n",
    "                #Backward propagation\n",
    "                dA3, loss = self.activation3.backward(Z3, mini_y)\n",
    "                dZ2 = self.FC3.backward(dA3)\n",
    "                dA2 = self.activation2.backward(dZ2)\n",
    "                dZ1 = self.FC2.backward(dA2)\n",
    "                dA1 = self.activation1.backward(dZ1)\n",
    "                dZ0 = self.FC1.backward(dA1)\n",
    "            if self.verbose:\n",
    "                A1 = self.FC1.forward(X)\n",
    "                z1 = self.activation1.forward(A1)\n",
    "                A2 = self.FC2.forward(z1)\n",
    "                z2= self.activation2.forward(A2)\n",
    "                A3 = self.FC3.forward(z2)\n",
    "                z3 = self.activation3.forward(A3)\n",
    "                self.loss_train.append(self.activation3.backward(Z3, y)[1])\n",
    "            if X_val is not None:\n",
    "                A1 = self.FC1.forward(X_val)\n",
    "                z1 = self.activation1.forward(A1)\n",
    "                A2 = self.FC2.forward(z1)\n",
    "                z2= self.activation2.forward(A2)\n",
    "                A3 = self.FC3.forward(z2)\n",
    "                z3 = self.activation3.forward(A3)\n",
    "                self.loss_val.append(self.activation3.backward(Z3, y_val)[1])\n",
    "                \n",
    "        def predict(Self, X):\n",
    "            A1 = self.FC1.forward(X)\n",
    "            z1 = self.activation1.forward(A1)\n",
    "            A2 = self.FC2.forward(z1)\n",
    "            z2= self.activation2.forward(A2)\n",
    "            A3 = self.FC3.forward(z2)\n",
    "            z3 = self.activation3.forward(A3)\n",
    "            return np.argmax(Z3, axis=1)\n",
    "            \n",
    "            \n",
    "                \n",
    "                \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c34a08d8-d924-406d-82ac-47771bbc2504",
   "metadata": {},
   "outputs": [],
   "source": [
    "#problem 9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "d85acd77-a993-4b6a-a39a-943d086f9e0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train.reshape(-1,784)\n",
    "X_test = X_test.reshape(-1, 784)\n",
    "X_train = X_train.astype(float)\n",
    "X_test = X_test.astype(float)\n",
    "X_train /= 255\n",
    "X_test /= 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "6cc2a855-8b46-4794-822e-04d2ef67cc05",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size = 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "bb5cc7c4-6ee7-4634-b722-f177cc873975",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\DoBUY\\anaconda3\\Lib\\site-packages\\sklearn\\preprocessing\\_encoders.py:868: FutureWarning: `sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "enc = OneHotEncoder(handle_unknown='ignore', sparse=False)\n",
    "y_train_one_hot = enc.fit_transform(y_train[:, np.newaxis])\n",
    "y_val_one_hot = enc.transform(y_val[:, np.newaxis])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "b506152c-723f-4a7e-826a-90a55a3a16fc",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "softmax() missing 1 required positional argument: 'z'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[35], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m SDNN \u001b[38;5;241m=\u001b[39m ScratchDeepNeuralNetworkClassifier(verbose \u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, epoch\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m, optimizer \u001b[38;5;241m=\u001b[39mAdaGrad, initializer \u001b[38;5;241m=\u001b[39mHeInitializer, activater \u001b[38;5;241m=\u001b[39m ReLU)\n\u001b[1;32m----> 2\u001b[0m SDNN\u001b[38;5;241m.\u001b[39mfit(X_train, y_train_one_hot, X_val, y_val_one_hot)\n",
      "Cell \u001b[1;32mIn[31], line 25\u001b[0m, in \u001b[0;36mScratchDeepNeuralNetworkClassifier.fit\u001b[1;34m(self, X, y, X_val, y_val)\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mactivation2 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mactivater()\n\u001b[0;32m     24\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mFC3 \u001b[38;5;241m=\u001b[39m FC(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_nodes2, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_output, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minitializer(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msigma), optimizer)\n\u001b[1;32m---> 25\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mactivation3 \u001b[38;5;241m=\u001b[39m softmax()\n\u001b[0;32m     27\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mepoch):\n\u001b[0;32m     28\u001b[0m     get_mini_batch \u001b[38;5;241m=\u001b[39m GetMiniBatch(X,y, batch_size \u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_size, seed\u001b[38;5;241m=\u001b[39mi)\n",
      "\u001b[1;31mTypeError\u001b[0m: softmax() missing 1 required positional argument: 'z'"
     ]
    }
   ],
   "source": [
    "SDNN = ScratchDeepNeuralNetworkClassifier(verbose =True, epoch=10, optimizer =AdaGrad, initializer =HeInitializer, activater = ReLU)\n",
    "SDNN.fit(X_train, y_train_one_hot, X_val, y_val_one_hot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22cf2461-536a-4702-9088-49df762e5f4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = SDNN.predict(X_val)\n",
    "accuracy_score(y_val, pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b171ad7-f574-47bc-b39c-1e98e3569be3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff99d8aa-a461-4064-8236-810951c92e0d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3478d704-54cc-4ff8-9e44-26435b4b1a8c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
